{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Read the Spark configuration for credentials, used in following cells.\r\n",
        "#\r\n",
        "# See Azure docs: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-create-spark-configuration\r\n",
        "# See Spark docs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html\r\n",
        "\r\n",
        "import os\r\n",
        "import traceback\r\n",
        "\r\n",
        "from pyspark.conf import SparkConf\r\n",
        "from pyspark.context import SparkContext\r\n",
        "\r\n",
        "storage_acct, storage_key = None, None\r\n",
        "\r\n",
        "try:\r\n",
        "    # Read the SparkConf for the Azure Storage credentials.\r\n",
        "    conf = SparkConf()\r\n",
        "    storage_acct = conf.get(\"spark.storage.acct\")\r\n",
        "    storage_key  = conf.get(\"spark.storage.key\")\r\n",
        "except Exception as e:\r\n",
        "    print(\"Exception reading SparkConf: {}\".format(str(e)))\r\n",
        "    traceback.print_exc()\r\n",
        "\r\n",
        "print(f'storage_acct:       {storage_acct}')\r\n",
        "print(f'storage_key prefix: {storage_key[0:10]}')\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Python classes in this cell, used in following cells.\r\n",
        "\r\n",
        "from azure.core.exceptions import ResourceExistsError\r\n",
        "from azure.core.exceptions import ResourceNotFoundError\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
        "from io import StringIO\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import pandas as pd\r\n",
        "import sys\r\n",
        "import traceback\r\n",
        "\r\n",
        "# ==============================================================================\r\n",
        "\r\n",
        "class Storage(object):\r\n",
        "    \"\"\" This class is an interface to Azure Storage. \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, opts={}):\r\n",
        "        acct_name = opts['acct']\r\n",
        "        acct_key  = opts['key']\r\n",
        "        acct_url  = 'https://{}.blob.core.windows.net/'.format(acct_name)\r\n",
        "\r\n",
        "        self.blob_service_client = BlobServiceClient(\r\n",
        "            account_url=acct_url, credential=acct_key)\r\n",
        "\r\n",
        "    def account_info(self):\r\n",
        "        return self.blob_service_client.get_account_information()\r\n",
        "\r\n",
        "    def list_containers(self):\r\n",
        "        clist = list()\r\n",
        "        try:\r\n",
        "            containers = self.blob_service_client.list_containers(include_metadata=True)\r\n",
        "            for container in containers:\r\n",
        "                clist.append(container)\r\n",
        "            return clist\r\n",
        "        except ResourceExistsError:\r\n",
        "            return clist\r\n",
        "\r\n",
        "    def create_container(self, cname):\r\n",
        "        try:\r\n",
        "            container_client = self.blob_service_client.get_container_client(cname)\r\n",
        "            container_client.create_container()\r\n",
        "            print('create_container: {}'.format(cname))\r\n",
        "        except ResourceExistsError:\r\n",
        "            pass\r\n",
        "\r\n",
        "    def delete_container(self, cname):\r\n",
        "        try:\r\n",
        "            container_client = self.blob_service_client.get_container_client(cname)\r\n",
        "            container_client.delete_container()\r\n",
        "            print('delete_container: {}'.format(cname))\r\n",
        "        except ResourceNotFoundError:\r\n",
        "            pass\r\n",
        "\r\n",
        "    def list_container(self, cname):\r\n",
        "        try:\r\n",
        "            container_client = self.blob_service_client.get_container_client(cname)\r\n",
        "            return container_client.list_blobs()\r\n",
        "        except ResourceExistsError:\r\n",
        "            return list()\r\n",
        "\r\n",
        "    def upload_blob_from_file(self, local_file_path, cname, blob_name, overwrite=True):\r\n",
        "        try:\r\n",
        "            blob_client = self.blob_service_client.get_blob_client(container=cname, blob=blob_name)\r\n",
        "            with open(local_file_path, \"rb\") as data:\r\n",
        "                blob_client.upload_blob(data, overwrite=overwrite)\r\n",
        "            print('upload_blob_from_file: {} -> {} {}'.format(local_file_path, cname, blob_name))\r\n",
        "            return True\r\n",
        "        except ResourceNotFoundError:\r\n",
        "            return False\r\n",
        "\r\n",
        "    def upload_blob_from_string(self, string_data, cname, blob_name, overwrite=True):\r\n",
        "        try:\r\n",
        "            blob_client = self.blob_service_client.get_blob_client(container=cname, blob=blob_name)\r\n",
        "            print('upload_blob_from_string: {} {}'.format(cname, blob_name))\r\n",
        "            blob_client.upload_blob(string_data, overwrite=overwrite)\r\n",
        "            return True\r\n",
        "        except ResourceNotFoundError:\r\n",
        "            return False\r\n",
        "\r\n",
        "    def download_blob(self, cname, blob_name, local_file_path):\r\n",
        "        try:\r\n",
        "            blob_client = self.blob_service_client.get_blob_client(container=cname, blob=blob_name)\r\n",
        "            with open(local_file_path, \"wb\") as download_file:\r\n",
        "                download_file.write(blob_client.download_blob().readall())\r\n",
        "            print('download_blob: {} {} -> {}'.format(cname, blob_name, local_file_path))\r\n",
        "        except ResourceNotFoundError:\r\n",
        "            pass\r\n",
        "\r\n",
        "    def download_blob_to_string(self, cname, blob_name):\r\n",
        "        blob_client = self.blob_service_client.get_blob_client(container=cname, blob=blob_name)\r\n",
        "        downloader = blob_client.download_blob(max_concurrency=1, encoding='UTF-8')\r\n",
        "        return downloader.readall()\r\n",
        "\r\n",
        "# ==============================================================================\r\n",
        "\r\n",
        "class AmCdbPartitionKeyStatsAggregator(object):\r\n",
        "    \"\"\"\r\n",
        "    This class is used to read and parse the Cosmos DB Partition Key Statistics\r\n",
        "    that are in Azure Storage via the automatic copy from Azure Monitor.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, stor: Storage):\r\n",
        "        self.stor = stor\r\n",
        "        self.folder_name = 'am-cdbpartitionkeystatistics'\r\n",
        "        self.output_columns = 'year,month,day,hour,minute,subscription,resource_group,account,region,database,container,pk,kb,gb'.split(',')\r\n",
        "        self.output_tuples = list()\r\n",
        "\r\n",
        "    def read_parse_storage_partition_key_stats(self):\r\n",
        "        \"\"\"\r\n",
        "        Return a Pandas DataFrame from the aggregated data of many blobs produced every 5-minutes.\r\n",
        "        \r\n",
        "        The raw JSON blob data looks like the following; one blob can have n-number of JSON lines:\r\n",
        "\r\n",
        "        { \"TimeGenerated\": \"2023-06-22T17:28:57.8885796Z\", \"AccountName\": \"gbbcjcdbnosql\", \"RegionName\": \"East US\", \"PartitionKey\": \"[\\\"triple|123\\\"]\", \"SizeKb\": 13277, \"DatabaseName\": \"dev\", \"CollectionName\": \"npm_graph\", \"_Internal_WorkspaceResourceId\": \"/subscriptions/xxx/resourcegroups/gbbcjcore/providers/microsoft.operationalinsights/workspaces/gbbcjmonitor\", \"Type\": \"CDBPartitionKeyStatistics\", \"TenantId\": \"ttt\", \"_ResourceId\": \"/SUBSCRIPTIONS/xxx/RESOURCEGROUPS/GBBCJCOSMOS/PROVIDERS/MICROSOFT.DOCUMENTDB/DATABASEACCOUNTS/GBBCJCDBNOSQL\"}\r\n",
        "        { \"TimeGenerated\": \"2023-06-22T17:29:07.7421709Z\", \"AccountName\": \"gbbcjcdbnosql\", \"RegionName\": \"East US\", \"PartitionKey\": \"[\\\"movie_seed\\\"]\", \"SizeKb\": 418946, \"DatabaseName\": \"dev\", \"CollectionName\": \"imdb_seed\", \"_Internal_WorkspaceResourceId\": \"/subscriptions/xxx/resourcegroups/gbbcjcore/providers/microsoft.operationalinsights/workspaces/gbbcjmonitor\", \"Type\": \"CDBPartitionKeyStatistics\", \"TenantId\": \"ttt\", \"_ResourceId\": \"/SUBSCRIPTIONS/xxx/RESOURCEGROUPS/GBBCJCOSMOS/PROVIDERS/MICROSOFT.DOCUMENTDB/DATABASEACCOUNTS/GBBCJCDBNOSQL\"}\r\n",
        "        { \"TimeGenerated\": \"2023-06-22T17:31:21.5493742Z\", \"AccountName\": \"gbbcjcdbnosql\", \"RegionName\": \"East US\", \"PartitionKey\": \"[\\\"triple|123\\\"]\", \"SizeKb\": 13278, \"DatabaseName\": \"dev\", \"CollectionName\": \"npm_graph\", \"_Internal_WorkspaceResourceId\": \"/subscriptions/xxx/resourcegroups/gbbcjcore/providers/microsoft.operationalinsights/workspaces/gbbcjmonitor\", \"Type\": \"CDBPartitionKeyStatistics\", \"TenantId\": \"ttt\", \"_ResourceId\": \"/SUBSCRIPTIONS/xxx/RESOURCEGROUPS/GBBCJCOSMOS/PROVIDERS/MICROSOFT.DOCUMENTDB/DATABASEACCOUNTS/GBBCJCDBNOSQL\"}\r\n",
        "        { \"TimeGenerated\": \"2023-06-22T17:31:11.6379249Z\", \"AccountName\": \"gbbcjcdbnosql\", \"RegionName\": \"East US\", \"PartitionKey\": \"[\\\"movie_seed\\\"]\", \"SizeKb\": 418946, \"DatabaseName\": \"dev\", \"CollectionName\": \"imdb_seed\", \"_Internal_WorkspaceResourceId\": \"/subscriptions/xxx/resourcegroups/gbbcjcore/providers/microsoft.operationalinsights/workspaces/gbbcjmonitor\", \"Type\": \"CDBPartitionKeyStatistics\", \"TenantId\": \"ttt\", \"_ResourceId\": \"/SUBSCRIPTIONS/xxx/RESOURCEGROUPS/GBBCJCOSMOS/PROVIDERS/MICROSOFT.DOCUMENTDB/DATABASEACCOUNTS/GBBCJCDBNOSQL\"}\r\n",
        "        { \"TimeGenerated\": \"2023-06-22T17:31:26.4979342Z\", \"AccountName\": \"gbbcjcdbnosql\", \"RegionName\": \"East US\", \"PartitionKey\": \"[\\\"movie_seed\\\"]\", \"SizeKb\": 418945, \"DatabaseName\": \"dev\", \"CollectionName\": \"imdb_seed\", \"_Internal_WorkspaceResourceId\": \"/subscriptions/xxx/resourcegroups/gbbcjcore/providers/microsoft.operationalinsights/workspaces/gbbcjmonitor\", \"Type\": \"CDBPartitionKeyStatistics\", \"TenantId\": \"ttt\", \"_ResourceId\": \"/SUBSCRIPTIONS/xxx/RESOURCEGROUPS/GBBCJCOSMOS/PROVIDERS/MICROSOFT.DOCUMENTDB/DATABASEACCOUNTS/GBBCJCDBNOSQL\"}\r\n",
        "        \"\"\"\r\n",
        "        try:\r\n",
        "            containers = self.stor.list_containers()\r\n",
        "            columns = self._standard_output_columns()\r\n",
        "            seq, download_count, tuples, max_tuples = 0, 0, list(), 1_000_000\r\n",
        "\r\n",
        "            # Iterate the containers, and the blobs in these containers.\r\n",
        "            # Read each blob into a Pandas DataFrame.\r\n",
        "            # Iterate the rows of each DataFrame, parse and collect into a list of tuples.\r\n",
        "            # Create an output DataFrame from the list of tuples, return the DataFrame.\r\n",
        "            # The DataFrame is then written to Azure Storage as CSV by the calling method.\r\n",
        "\r\n",
        "            for c in containers:\r\n",
        "                if c.name.startswith(self.folder_name):\r\n",
        "                    print('container: {}'.format(c.name))\r\n",
        "                    blobs = self.stor.list_container(c.name)\r\n",
        "                    for blob_idx, blob in enumerate(blobs):\r\n",
        "                        if len(tuples) < max_tuples:\r\n",
        "                            print('container: {} blob name: {} size: {}'.format(\r\n",
        "                                blob.container, blob.name, blob.size))\r\n",
        "                            download_count = download_count + 1\r\n",
        "                            blob_text = self.stor.download_blob_to_string(c.name, blob.name)\r\n",
        "                            sio = StringIO(blob_text)\r\n",
        "                            time_values = self._parse_year_month_day_hour_minute_values(blob.name, 'blob')\r\n",
        "                            df = pd.read_json(sio, lines=True)\r\n",
        "                            for index, df_row in df.iterrows():\r\n",
        "                                subs_rg = self._parse_subscription_and_resource_group(df_row['_ResourceId'])\r\n",
        "                                # for each blob JSON row produce a CSV row, append to tuples\r\n",
        "                                values = list()\r\n",
        "                                values.append(str(time_values[0]))\r\n",
        "                                values.append(str(time_values[1]))\r\n",
        "                                values.append(str(time_values[2]))\r\n",
        "                                values.append(str(time_values[3]))\r\n",
        "                                values.append(str(time_values[4]))\r\n",
        "                                values.append(str(subs_rg[0]))\r\n",
        "                                values.append(str(subs_rg[1]))\r\n",
        "                                values.append(str(df_row['AccountName']))\r\n",
        "                                values.append(str(df_row['RegionName']))\r\n",
        "                                values.append(str(df_row['DatabaseName']))\r\n",
        "                                values.append(str(df_row['CollectionName']))\r\n",
        "                                values.append(self._scrub_partition_key(df_row['PartitionKey']))\r\n",
        "                                values.append(str(df_row['SizeKb']))\r\n",
        "                                values.append(str(self._kb_to_gb(df_row['SizeKb'])))\r\n",
        "                                tuples.append(tuple(values))\r\n",
        "\r\n",
        "            print('blobs processed:  {}'.format(download_count))\r\n",
        "            print('parsed row count: {}'.format(len(tuples)))\r\n",
        "            return pd.DataFrame(tuples, columns=columns)\r\n",
        "\r\n",
        "        except Exception as e:\r\n",
        "            print(\"exception: {}\".format(str(e)))\r\n",
        "            traceback.print_exc()\r\n",
        "\r\n",
        "    def _standard_output_columns(self):\r\n",
        "        fields = 'year,month,day,hour,minute,subscription,resource_group,account,region,database,container,pk,kb,gb'\r\n",
        "        return fields.split(',')\r\n",
        "\r\n",
        "    def _parse_year_month_day_hour_minute_values(self, filename: str, blob_or_file: str):\r\n",
        "        if blob_or_file == 'blob':\r\n",
        "            int_idx = 2\r\n",
        "        else:\r\n",
        "            int_idx = 1\r\n",
        "        try:\r\n",
        "            fits_ymdh_pattern = False\r\n",
        "            if '/y' in filename:\r\n",
        "                if '/m' in filename:\r\n",
        "                    if '/d' in filename:\r\n",
        "                        if '/h' in filename:\r\n",
        "                            fits_ymdh_pattern = True\r\n",
        "            if fits_ymdh_pattern == True:\r\n",
        "                tokens = filename.split('/')\r\n",
        "                for token_idx, token in enumerate(tokens):\r\n",
        "                    if token.startswith('y'):\r\n",
        "                        if tokens[token_idx + 1].startswith('m'):\r\n",
        "                            if tokens[token_idx + 2].startswith('d'):\r\n",
        "                                if tokens[token_idx + 3].startswith('h'):\r\n",
        "                                    if tokens[token_idx + 4].startswith('m'):\r\n",
        "                                        time_values = list()\r\n",
        "                                        time_values.append(int(tokens[token_idx][int_idx:]))\r\n",
        "                                        time_values.append(int(tokens[token_idx + 1][int_idx:]))\r\n",
        "                                        time_values.append(int(tokens[token_idx + 2][int_idx:]))\r\n",
        "                                        time_values.append(int(tokens[token_idx + 3][int_idx:]))\r\n",
        "                                        time_values.append(int(tokens[token_idx + 4][int_idx:]))\r\n",
        "                                        return time_values\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "        return None\r\n",
        "\r\n",
        "    def _scrub_partition_key(self, pk):\r\n",
        "        return pk.replace('[','').replace(']','').replace(\"\\\"\",'')\r\n",
        "\r\n",
        "    def _parse_subscription_and_resource_group(self, resource_id):\r\n",
        "        tokens = resource_id.split('/')\r\n",
        "        values = '?,?'.split(',')\r\n",
        "        try:\r\n",
        "            for idx, token in enumerate(tokens):\r\n",
        "                if token.upper() == 'SUBSCRIPTIONS':\r\n",
        "                    values[0] = tokens[idx + 1]\r\n",
        "                if token.upper() == 'RESOURCEGROUPS':\r\n",
        "                    values[1] = tokens[idx + 1].lower()\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "        return values\r\n",
        "\r\n",
        "    def _kb_to_gb(self, kb_str):\r\n",
        "        try:\r\n",
        "            kb = float(kb_str)\r\n",
        "            return kb / 1024.0 / 1024.0\r\n",
        "        except:\r\n",
        "            return float(-1.0)\r\n",
        "\r\n",
        "def read_parse_storage_partition_key_stats():\r\n",
        "    stor = _get_storage_object()\r\n",
        "    agg = AmCdbPartitionKeyStatsAggregator(stor)\r\n",
        "    df = agg.read_parse_storage_partition_key_stats()\r\n",
        "    cname, blobname = 'wrangled', 'partition_key_stats.csv'\r\n",
        "    print(f'writing DataFrame to container: {cname} blob: {blobname}')\r\n",
        "    stor.upload_blob_from_string(df.to_csv(), cname, blobname)\r\n",
        "    print(f'writing DataFrame to tmp/{blobname}')\r\n",
        "    df.to_csv(f'tmp/{blobname}', encoding='utf-8', index=False)\r\n",
        "\r\n",
        "# \"private\" methods, not directly invoked by the command-line, below:\r\n",
        "\r\n",
        "def _get_storage_object():\r\n",
        "    opts = dict()\r\n",
        "    opts['acct'] = storage_acct  # os.environ['AZURE_AML_STORAGE_NAME']\r\n",
        "    opts['key']  = storage_key   # os.environ['AZURE_AML_STORAGE_KEY']\r\n",
        "    stor = Storage(opts)\r\n",
        "    print(stor.account_info())\r\n",
        "    return stor\r\n",
        "\r\n",
        "def _overwrite():\r\n",
        "    return True\r\n",
        "\r\n",
        "\r\n",
        "print(f'storage_acct: {storage_acct}')\r\n",
        "print('define classes cell completed')\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Main logic. Use the classes and code defined in the above cells.\r\n",
        "# Read the many Azure Storage Blobs populated from Azure Monitor\r\n",
        "# regarding Cosmos DB partition key stats.\r\n",
        "# Merge these into a Spark DataFrame (df).\r\n",
        "\r\n",
        "stor = _get_storage_object()\r\n",
        "agg = AmCdbPartitionKeyStatsAggregator(stor)\r\n",
        "df = agg.read_parse_storage_partition_key_stats()\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of the DataFrame, and upload it to another\r\n",
        "# Azure Storage blob in CSV format.\r\n",
        "\r\n",
        "print('df shape: {}'.format(df.shape))\r\n",
        "cname, blobname = 'wrangled', 'partition_key_stats_from_synapse.csv'\r\n",
        "print(f'writing DataFrame to container: {cname} blob: {blobname}')\r\n",
        "stor.upload_blob_from_string(df.to_csv(), cname, blobname)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}